{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0d3c19fe",
            "metadata": {},
            "source": [
                "# RECEIPT DETECTION OCR - RESEARCH PIPELINE\n",
                "**Objective:** Komparasi arsitektur OCR (Tesseract, EasyOCR, PaddleOCR, MMOCR, DocTR) untuk ekstraksi data struk belanja dengan optimasi akurasi metrik absolut (CER/WER) pada level karakter dan F1-Score pada level *field* informasi.\n",
                "\n",
                "---\n",
                "\n",
                "### 1. SETUP & IMPORTS\n",
                "*inisialisasi environment. Pastikan Runtime diatur ke GPU (T4/V100/A100) sebelum menjalankan.*\n",
                "\n",
                "- **[Code]** Mount Google Drive untuk akses dataset.\n",
                "- **[Code]** Install dependencies utama:\n",
                "  `!apt install tesseract-ocr`\n",
                "  `!pip install pytesseract easyocr paddleocr paddlepaddle-gpu python-doctr[tf] mmocr`\n",
                "- **[Code]** Install dependencies evaluasi & NLP:\n",
                "  `!pip install jiwer rapidfuzz opencv-python-headless matplotlib seaborn`\n",
                "- **[Code]** Import libraries & set Random Seed untuk reproduktibilitas riset.\n",
                "\n",
                "---\n",
                "\n",
                "### 2. EDA & DATA CLEANING\n",
                "*Fase validasi input. Model yang baik akan gagal jika diberi data kotor atau anotasi yang salah.*\n",
                "\n",
                "- **[Markdown]** Analisis distribusi dataset (Rasio data mulus vs. data terdistorsi).\n",
                "- **[Code]** Visualisasi sampel dataset Geometri (Skewed/Tilted), Pencahayaan (Low-light/Faded), dan Struktural (Cutted/Occluded).\n",
                "- **[Code]** Deduplikasi menggunakan *Perceptual Hashing* (pHash) untuk membuang gambar identik.\n",
                "- **[Code]** Pembagian dataset terspesifikasi (Stratified Split: 70% Train, 15% Val, 15% Test) dengan proporsi tingkat kesulitan yang merata.\n",
                "- **[Code]** Standarisasi format Ground Truth ke dalam JSON tunggal.\n",
                "\n",
                "---\n",
                "\n",
                "### 3. PREPROCESSING EXPERIMENTS\n",
                "*Implementasi percabangan preprocessing. Model Deep Learning (Branch A/B) vs Traditional (Branch C).*\n",
                "\n",
                "- **[Markdown]** Metodologi Koreksi Geometri dan Pencahayaan.\n",
                "- **[Code]** Fungsi Geometric Correction (Deskew & Perspective Transform menggunakan Canny Edge + Hough Lines).\n",
                "- **[Code]** Fungsi Illumination Correction (CLAHE pada *color space* LAB).\n",
                "- **[Code]** Binarization Test (Adaptive Gaussian Thresholding) - **Khusus untuk Tesseract**.\n",
                "- **[Code]** Visualisasi komparatif: Original vs. Pipeline DL (Geometri+CLAHE) vs. Pipeline Tesseract (Geometri+Binarization).\n",
                "\n",
                "---\n",
                "\n",
                "### 4. OCR MODEL BENCHMARKING (INTI RESEARCH)\n",
                "*Fase pengujian model secara terisolasi menggunakan data uji (Test Set) yang telah diproses.*\n",
                "\n",
                "- **[Code]** Inisialisasi Class Wrapper untuk penyeragaman output format: `[{\"box\": [x,y,w,h], \"text\": \"...\", \"conf\": 0.9}]`.\n",
                "- **[Code]** 4.1 Pipeline Tesseract (Input: Branch C Preprocessing)\n",
                "- **[Code]** 4.2 Pipeline EasyOCR (Input: Branch A+B Preprocessing)\n",
                "- **[Code]** 4.3 Pipeline PaddleOCR (Input: Branch A+B Preprocessing)\n",
                "- **[Code]** 4.4 Pipeline MMOCR (Input: Branch A+B Preprocessing)\n",
                "- **[Code]** 4.5 Pipeline DocTR (Input: Branch A+B Preprocessing)\n",
                "- **[Code]** 4.6 Evaluasi Komparatif:\n",
                "  - Kalkulasi metrik Word Error Rate (WER) & Character Error Rate (CER) menggunakan library `jiwer`.\n",
                "  - Kalkulasi *Inference Time* (FPS).\n",
                "  - Pembuatan Tabel Pandas dan visualisasi *Bar Chart* komparasi performa.\n",
                "\n",
                "---\n",
                "\n",
                "### 5. POST-PROCESSING & INFORMATION EXTRACTION\n",
                "*Fase penerjemahan kotak teks acak menjadi informasi terstruktur (Key-Value).*\n",
                "\n",
                "- **[Markdown]** Logika *Spatial Sorting* untuk rekonstruksi baris teks (Mengatasi teks miring minor).\n",
                "- **[Code]** Implementasi Algoritma Y-Axis Tolerance Sorting (Pengelompokan baris berdasarkan delta $Y$).\n",
                "- **[Code]** Implementasi Algoritma X-Axis Sorting (Pengurutan kiri-kanan per baris).\n",
                "- **[Code]** Parsing JSON/Dict dari raw OCR text hasil pemenang tahap 4.\n",
                "- **[Code]** Regex & Fuzzy matching ekstraksi informasi:\n",
                "  - Ekstraksi **Tanggal** (Regex variasi format tanggal lokal/internasional).\n",
                "  - Ekstraksi **Total Belanja** (Pencarian anchor \"Total\"/\"Amount\" dilanjut validasi format mata uang).\n",
                "  - Ekstraksi **Merchant** (Ekstraksi baris teratas + `rapidfuzz` string matching ke database).\n",
                "- **[Code]** Evaluasi presisi ekstraksi (Berapa persen field Total/Tanggal yang benar-benar akurat 100%?).\n",
                "\n",
                "---\n",
                "\n",
                "### 6. KESIMPULAN & EXPORT\n",
                "*Pembungkusan seluruh riset menjadi modul yang siap diintegrasikan oleh tim Software Engineering.*\n",
                "\n",
                "- **[Markdown]** Justifikasi pemilihan model final (Menyeimbangkan CER, Waktu Eksekusi, dan Akurasi Ekstraksi).\n",
                "- **[Markdown]** Analisis Error (Pada kasus seperti apa pipeline ini masih gagal?).\n",
                "- **[Code]** Penyatuan fungsi menjadi satu Class Pipeline end-to-end `class ReceiptScanner`.\n",
                "- **[Code]** Export pipeline terbaik menjadi class Python (`.py`) atau model ter-pickle siap pakai.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-setup-deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 1 â€” SETUP DEPENDENCIES\n",
                "# STATUS: Semua package di bawah sudah terinstall di .venv\n",
                "#         (Python 3.12 | Kernel: \"Python 3.12 (OCR Research)\")\n",
                "#\n",
                "# Jalankan cell ini hanya jika environment direset / baru clone.\n",
                "# ============================================================\n",
                "\n",
                "import subprocess, sys\n",
                "\n",
                "def pip_install(packages: str, extra_index: str = \"\"):\n",
                "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + packages.split()\n",
                "    if extra_index:\n",
                "        cmd += [\"--index-url\", extra_index]\n",
                "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
                "    print(result.stdout[-2000:] if result.stdout else \"\")\n",
                "    if result.returncode != 0:\n",
                "        print(\"[ERROR]\", result.stderr[-500:])\n",
                "    return result.returncode\n",
                "\n",
                "# â”€â”€ Step 1: PyTorch + CUDA 12.1 (NVIDIA GPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"[1/3] Installing PyTorch CUDA 12.1 ...\")\n",
                "pip_install(\n",
                "    \"torch torchvision torchaudio\",\n",
                "    extra_index=\"https://download.pytorch.org/whl/cu121\"\n",
                ")\n",
                "\n",
                "# â”€â”€ Step 2: Core OCR engines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# NOTE: MMOCR belum support Python 3.12 di Windows secara resmi.\n",
                "print(\"[2/3] Installing OCR engines ...\")\n",
                "pip_install(\n",
                "    \"pytesseract easyocr paddleocr paddlepaddle-gpu \"\n",
                "    \"python-doctr[torch]\"\n",
                ")\n",
                "\n",
                "# â”€â”€ Step 3: Evaluation & utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"[3/3] Installing evaluation & utility packages ...\")\n",
                "pip_install(\n",
                "    \"jiwer rapidfuzz imagehash Pillow \"\n",
                "    \"opencv-python matplotlib seaborn \"\n",
                "    \"pandas scikit-learn tqdm ipykernel requests\"\n",
                ")\n",
                "\n",
                "print(\"\\nâœ…  Setup selesai. Restart kernel lalu jalankan Cell 2.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 2 â€” IMPORT LIBRARIES & GLOBAL CONFIG\n",
                "# ============================================================\n",
                "\n",
                "# â”€â”€ Standard Library â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import os\n",
                "import re\n",
                "import json\n",
                "import io\n",
                "import time\n",
                "import random\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "# â”€â”€ Numerik & Visualisasi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm   # Menggunakan tqdm standar agar tidak kena error IProgress\n",
                "\n",
                "# â”€â”€ Networking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import requests\n",
                "\n",
                "# â”€â”€ Image Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import cv2\n",
                "from PIL import Image\n",
                "import imagehash                        # pHash deduplication\n",
                "\n",
                "# â”€â”€ OCR Engines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import pytesseract\n",
                "# âš ï¸  Sesuaikan path jika Tesseract diinstall di lokasi berbeda\n",
                "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
                "\n",
                "import easyocr\n",
                "from paddleocr import PaddleOCR\n",
                "from doctr.io import DocumentFile\n",
                "from doctr.models import ocr_predictor\n",
                "\n",
                "# âš ï¸  MMOCR: Belum support Python 3.12 + Windows secara resmi.\n",
                "# from mmocr.apis import MMOCRInferencer\n",
                "\n",
                "# â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from jiwer import wer, cer\n",
                "from rapidfuzz import fuzz, process\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# â”€â”€ CUDA check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "import torch\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"ğŸ–¥ï¸  Device  : {DEVICE}\")\n",
                "if DEVICE == \"cuda\":\n",
                "    print(f\"ğŸ®  GPU     : {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"ğŸ’¾  VRAM    : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
                "\n",
                "# â”€â”€ Global Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if DEVICE == \"cuda\":\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"\\nâœ…  Semua library berhasil diimport.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-load-dataset",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 3 â€” LOAD DATASET (LOCAL)\n",
                "# Path otomatis dari lokasi notebook â€” tidak hardcoded username.\n",
                "# ============================================================\n",
                "\n",
                "PROJECT_ROOT = Path.cwd()\n",
                "DATASET_DIR  = PROJECT_ROOT / \"dataset\"\n",
                "IMG_DIR      = DATASET_DIR / \"images\"\n",
                "ANNOT_DIR    = DATASET_DIR / \"annotations\"\n",
                "GT_FILE      = ANNOT_DIR / \"ground_truth.json\"\n",
                "\n",
                "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}\n",
                "\n",
                "print(f\"ğŸ“‚  Project root : {PROJECT_ROOT}\")\n",
                "print(f\"ğŸ“‚  Dataset root : {DATASET_DIR}\")\n",
                "\n",
                "image_paths: list[Path] = []\n",
                "if IMG_DIR.exists():\n",
                "    image_paths = sorted(\n",
                "        p for p in IMG_DIR.rglob(\"*\") if p.suffix.lower() in IMG_EXTS\n",
                "    )\n",
                "else:\n",
                "    print(f\"âš ï¸  Folder gambar tidak ditemukan: {IMG_DIR}\")\n",
                "\n",
                "print(f\"ğŸ–¼ï¸  Total gambar : {len(image_paths)}\")\n",
                "\n",
                "ground_truth: dict = {}\n",
                "if GT_FILE.exists():\n",
                "    with open(GT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
                "        ground_truth = json.load(f)\n",
                "    print(f\"ğŸ“‹  Ground truth : {len(ground_truth)} entri dari {GT_FILE.name}\")\n",
                "else:\n",
                "    print(f\"âš ï¸  Ground truth belum ada â€” akan dibuat di Cell EDA.\")\n",
                "\n",
                "if image_paths:\n",
                "    summary: dict[str, int] = {}\n",
                "    for p in image_paths:\n",
                "        summary[p.parent.name] = summary.get(p.parent.name, 0) + 1\n",
                "    summary_df = pd.DataFrame(\n",
                "        list(summary.items()), columns=[\"Kategori\", \"Jumlah Gambar\"]\n",
                "    ).sort_values(\"Jumlah Gambar\", ascending=False).reset_index(drop=True)\n",
                "    print(\"\\nğŸ“Š  Distribusi dataset per kategori:\")\n",
                "    display(summary_df)\n",
                "else:\n",
                "    print(\"\\nâ„¹ï¸  Dataset lokal kosong â€” gunakan Cell EDA untuk fetch dari HuggingFace.\")\n",
                "\n",
                "print(\"\\nâœ…  Load dataset selesai.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-eda-fetch",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 4 â€” TAHAP 1: EDA â€” FETCH DATA DARI HUGGING FACE\n",
                "# Dataset: naver-clova-ix/cord-v1\n",
                "# âœ… Telah diuji di terminal â€” endpoint & field mapping verified\n",
                "# ============================================================\n",
                "\n",
                "BASE_URL = \"https://datasets-server.huggingface.co\"\n",
                "DATASET  = \"naver-clova-ix%2Fcord-v1\"\n",
                "FETCH_N  = 100   # jumlah sampel yang diambil\n",
                "\n",
                "def fetch_hf_dataset(n: int = 100) -> list[dict]:\n",
                "    \"\"\"\n",
                "    Fetch rows dari HuggingFace Datasets Server.\n",
                "    /rows kadang 500 â€” otomatis fallback ke /first-rows.\n",
                "    \"\"\"\n",
                "    print(f\"Mengambil {n} baris dari HuggingFace API (CORD-v1)...\")\n",
                "\n",
                "    url = (\n",
                "        f\"{BASE_URL}/rows\"\n",
                "        f\"?dataset={DATASET}&config=default&split=train\"\n",
                "        f\"&offset=0&length={n}\"\n",
                "    )\n",
                "    r = requests.get(url, timeout=30)\n",
                "\n",
                "    if r.status_code != 200:\n",
                "        print(f\"âš ï¸  /rows gagal ({r.status_code}), fallback ke /first-rows...\")\n",
                "        url = f\"{BASE_URL}/first-rows?dataset={DATASET}&config=default&split=train\"\n",
                "        r = requests.get(url, timeout=30)\n",
                "        r.raise_for_status()\n",
                "\n",
                "    return r.json().get(\"rows\", [])\n",
                "\n",
                "\n",
                "# â”€â”€ Proses Ingesti Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "raw_rows = fetch_hf_dataset(FETCH_N)\n",
                "dataset_processed = []\n",
                "\n",
                "if raw_rows:\n",
                "    for item in tqdm(raw_rows, desc=\"Memproses Baris Data\"):\n",
                "        row_content = item[\"row\"]\n",
                "\n",
                "        # image field adalah dict: {'src': url, 'height': int, 'width': int}\n",
                "        image_field = row_content[\"image\"]\n",
                "        image_url   = image_field[\"src\"] if isinstance(image_field, dict) else image_field\n",
                "\n",
                "        # ground_truth adalah JSON string\n",
                "        gt_raw = row_content[\"ground_truth\"]\n",
                "        gt     = json.loads(gt_raw) if isinstance(gt_raw, str) else gt_raw\n",
                "\n",
                "        dataset_processed.append({\n",
                "            \"image_url\"    : image_url,\n",
                "            \"image_width\"  : image_field.get(\"width\",  0) if isinstance(image_field, dict) else 0,\n",
                "            \"image_height\" : image_field.get(\"height\", 0) if isinstance(image_field, dict) else 0,\n",
                "            \"ground_truth\" : gt,\n",
                "            \"label\"        : \"clear\",\n",
                "        })\n",
                "\n",
                "    print(f\"\\nâœ…  Berhasil memuat {len(dataset_processed)} sampel.\")\n",
                "\n",
                "    # â”€â”€ Visualisasi Sampel EDA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    sample = dataset_processed[0]\n",
                "    img_bytes   = requests.get(sample[\"image_url\"], timeout=15).content\n",
                "    img_preview = Image.open(io.BytesIO(img_bytes))\n",
                "\n",
                "    plt.figure(figsize=(6, 8))\n",
                "    plt.imshow(img_preview)\n",
                "    plt.title(\"Sampel Dataset CORD-v1\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "\n",
                "    # â”€â”€ Analisis Struktur Ground Truth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    gt_keys = list(sample[\"ground_truth\"].keys())\n",
                "    print(f\"ğŸ“‹  Field GT yang ditemukan : {gt_keys}\")\n",
                "    print(f\"ğŸ“  Dimensi gambar          : {sample['image_width']}Ã—{sample['image_height']} px\")\n",
                "\n",
                "else:\n",
                "    print(\"âŒ  Tidak ada data yang berhasil diproses.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cell-cleaning-split",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CELL 5 â€” TAHAP 2: DATA CLEANING (pHASH DEDUP & SPLIT)\n",
                "# âœ… Telah diuji di terminal â€” logika verified\n",
                "# ============================================================\n",
                "\n",
                "print(\"Memulai Tahap 2: Data Cleaning & Stratified Splitting...\")\n",
                "print(\"-\" * 55)\n",
                "\n",
                "final_clean = []\n",
                "seen_hashes: dict = {}\n",
                "dup_count   = 0\n",
                "\n",
                "# â”€â”€ 1. Deduplikasi Visual (Perceptual Hashing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Hamming Distance <= 2 dianggap duplikat visual\n",
                "for sample in tqdm(dataset_processed, desc=\"Deduplikasi pHash\"):\n",
                "    try:\n",
                "        img_bytes = requests.get(sample[\"image_url\"], timeout=10).content\n",
                "        img_obj   = Image.open(io.BytesIO(img_bytes))\n",
                "        h         = imagehash.phash(img_obj)\n",
                "\n",
                "        if any((h - k) <= 2 for k in seen_hashes):\n",
                "            dup_count += 1\n",
                "        else:\n",
                "            seen_hashes[h] = True\n",
                "            final_clean.append(sample)\n",
                "    except Exception:\n",
                "        continue  # skip gambar yang gagal diload\n",
                "\n",
                "print(f\"\\nHasil Cleaning:\")\n",
                "print(f\"   - Duplikat dihapus : {dup_count}\")\n",
                "print(f\"   - Dataset unik     : {len(final_clean)}\")\n",
                "\n",
                "# â”€â”€ 2. Stratified Split 70 / 15 / 15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "if len(final_clean) >= 10:\n",
                "    train_data, temp_data = train_test_split(\n",
                "        final_clean, test_size=0.30, random_state=SEED\n",
                "    )\n",
                "    val_data, test_data = train_test_split(\n",
                "        temp_data, test_size=0.50, random_state=SEED\n",
                "    )\n",
                "\n",
                "    print(f\"\\nStatistik Pembagian Dataset:\")\n",
                "    print(f\"   - Training Set   : {len(train_data)}\")\n",
                "    print(f\"   - Validation Set : {len(val_data)}\")\n",
                "    print(f\"   - Testing Set    : {len(test_data)}\")\n",
                "else:\n",
                "    print(\"âš ï¸  Data tidak mencukupi untuk splitting (min. 10 sampel).\")\n",
                "\n",
                "print(\"-\" * 55)\n",
                "print(\"âœ…  Tahap 2 Selesai. Dataset siap untuk Preprocessing Experiments.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.12 (OCR Research)",
            "language": "python",
            "name": "ocr-research"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}